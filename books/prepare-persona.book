# Prepare Persona

-   PIPELINE URL `https://promptbook.studio/promptbook/prepare-persona.book`
-   INPUT PARAMETER `{availableModels}` List of available model names together with their descriptions as JSON
-   INPUT PARAMETER `{personaDescription}` Description of the persona
-   OUTPUT PARAMETER `{modelsRequirements}` Specific requirements for the model

## Make modelRequirements

-   FORMAT JSON

```markdown
You are an experienced AI engineer, you need to find the best models for virtual assistants:

## Example

\`\`\`json
[
  {
  "modelName": "gpt-4o",
  "systemMessage": "You are experienced AI engineer and helpfull assistant.",
  "temperature": 0.7
  },
  {
    "modelName": "claude-3-5-sonnet",
    "systemMessage": "You are a friendly and knowledgeable chatbot.",
    "temperature": 0.5
  }
]
\`\`\`

## Instructions

-   Your output format is JSON array
-   Sort best-fitting models first
-   Omit any models that are not suitable
-   Write just the JSON, no other text should be present
-   Array contain items with following keys:
    -   `modelName`: The name of the model to use
    -   `systemMessage`: The system message to provide context to the model
    -   `temperature`: The sampling temperature to use

### Key `modelName`

Here are the available models:

\`\`\`json
{availableModels}
\`\`\`

### Key `systemMessage`

The system message is used to communicate instructions or provide context to the model at the beginning of a conversation. It is displayed in a different format compared to user messages, helping the model understand its role in the conversation. The system message typically guides the model's behavior, sets the tone, or specifies desired output from the model. By utilizing the system message effectively, users can steer the model towards generating more accurate and relevant responses.

For example:

> You are an experienced AI engineer and helpful assistant.

> You are a friendly and knowledgeable chatbot.

### Key `temperature`

The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use log probability to automatically increase the temperature until certain thresholds are hit.

You can pick a value between 0 and 2. For example:

-   `0.1`: Low temperature, extremely conservative and deterministic
-   `0.5`: Medium temperature, balanced between conservative and creative
-   `1.0`: High temperature, creative and bit random
-   `1.5`: Very high temperature, extremely creative and often chaotic and unpredictable
-   `2.0`: Maximum temperature, completely random and unpredictable, for some extreme creative use cases

# The assistant

Take this description of the persona:

> {personaDescription}
```

`-> {modelsRequirements}`
