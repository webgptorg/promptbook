[x]

[âœ¨ðŸ’ž] Create component `LlmChat` component

This should show same chat as [Chat component](/src/book-components/Chat/Chat/Chat.tsx) but allow to chat with LLM modal

-   Put in under `/src/book-components/Chat/LlmChat/LlmChat.tsx`
-   Use [Chat component](/src/book-components/Chat/Chat/Chat.tsx) internally
-   Derive `LlmChatProps` from `ChatProps`
-   You wont pass `messages`, `onMessage`, `tasksProgress` and `participants`
    -   `messages` and `onMessage`
    -   `onChange` will be still present `LlmChatProps` will report the changes to callback
    -   `tasksProgress` will be handled internally
    -   `participants` will be generated from `llmTools`
    -   There will be `llmTools` prop
-   Internally in the component you will use `useState` to manage `messages` and `tasksProgress` and show them via `Chat` component
-   You are chatting with the LLM passed via `llmTools`, chatting via method `callChatModel`
-   Make also the preview component `LlmChatPreview`
    -   Use `MockedEchoLlmExecutionTools` in the preview
    -   Look at folder `/book-components/src/components`
    -   Look how other previews are made
        -   Make a preview component
        -   Also a `component.yaml` file
        -   Register it in `/book-components/src/components/ComponentPreview.tsx`
-   Keep in mind DRY (Don't Repeat Yourself) principle

---

[x]

[âœ¨ðŸ’ž] Add `persistenceKey` prop to `LlmChat`

-   When not set `LlmChat` behaves same as now
-   When set, persist the conversation in the `localStorage` under that key
-   When user refreshes the page the conversation stays there until reset
-   Keep in mind DRY (Don't Repeat Yourself) principle

---

[x]

[âœ¨ðŸ’ž] Type [`LlmExecutionTools`](/src/execution/LlmExecutionTools.ts) should have property `readonly profile?: ChatParticipant`

-   This should represent the model as the virtual persona, for example OpenAI should have OpenAI image, their green color,...
-   Do this profile for each `LlmExecutionTools` and also for every function which returns `LlmExecutionTools`
-   Use this `profile` in `participants` in [`LlmChat`](/src/book-components/Chat/LlmChat/LlmChat.tsx) component
-   Keep in mind DRY (Don't Repeat Yourself) principle

---

[x]

[âœ¨ðŸ’ž] In [`LlmChatPreview`](/book-components/src/components/llm-chat/LlmChatPreview.tsx) allow to chat with `OpenAiExecutionTools`

-   In the "LLM Chat Scenario:" allow to pick between the "Mock Chat" and "OpenAI Chat"
-   Add field for api key for Open AI
-   Persist this key in the LocalStorage
-   Keep in mind DRY (Don't Repeat Yourself) principle

---

[-] <- [ðŸ§ ]

[âœ¨ðŸ’ž] Refactor [llmProviderProfiles.ts](/src/llm-providers/_common/profiles/llmProviderProfiles.ts) - each profile must be alongside the provider definition, file llmProviderProfiles.ts should be disolved

---

[-]

[âœ¨ðŸ’ž] In [`LlmChatPreview`](/book-components/src/components/llm-chat/LlmChatPreview.tsx) allow to chat with various LLM providers

-   Alongside the "OpenAI" add all other `LlmExecutionTools` available here, each should be added in `<LlmChat />` as scenario
-   For each allow to setup the configuration, use `$llmToolsMetadataRegister` and `$llmToolsRegister`
-   To construct the llm execution tools, use the constructors from `llmToolsRegister`
-   The existing `OpenAiExecutionTools` should be made with same system
-   Keep in mind DRY (Don't Repeat Yourself) principle
