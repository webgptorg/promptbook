[ ]

[‚ú®üê®] Implement streaming in [`LlmExecutionTools`](/src/execution/LlmExecutionTools.ts)

-   Implement as smoothly as possible the streaming of responses from LLMs in the `LlmExecutionTools` class
-   Modify the `LlmExecutionTools` type
-   Do implementation for each `LlmExecutionTools` and also for every part of the system that works with `LlmExecutionTools`
-   Keep in mind DRY (Don't Repeat Yourself) principle
-   Integrate it seamlessly into `callChatModel` and `ChatPromptResult`
-   do not create extra methods and interfaces for the streaming and non-streaming
-   Allow to pass callback as second parameter of `callChatModel` which can recieve partial `ChatPromptResult`
    -   `public callChatModel(prompt: ChatPrompt, onPartialResult?: (result: Partial<ChatPromptResult & Pick<ChatPromptResult,'content'>>) => void): Promise<ChatPromptResult>`
    -   In the implementation method `callChatModel` can call either streaming / non-streaming whether `onPartialResult` is used or not

---

[ ]

[‚ú®üê®] qux

---

[ ]

[‚ú®üê®] qux

---

[ ]

[‚ú®üê®] qux
