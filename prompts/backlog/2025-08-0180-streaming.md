[-] <- [ğŸ§ ]

[âœ¨ğŸ¨] Implement streaming in [`LlmExecutionTools`](/src/execution/LlmExecutionTools.ts)

-   Implement as smoothly as possible the streaming of responses from LLMs in the `LlmExecutionTools` class
-   Do implementation for each `LlmExecutionTools` and also for every part of the system that works with `LlmExecutionTools`
-   Integrate it seamlessly into `callChatModel` and `ChatPromptResult`
-   Modify the `LlmExecutionTools` type
-   Do not create extra methods and interfaces for the streaming and non-streaming
-   Allow to pass callback as second parameter of `callChatModel` which can recieve partial `ChatPromptResult`
    -   `public callChatModel(prompt: ChatPrompt, onPartialResult?: (result: Partial<ChatPromptResult & Pick<ChatPromptResult,'content'>>) => void): Promise<ChatPromptResult>`
    -   In the implementation method `callChatModel` can call either streaming / non-streaming whether `onPartialResult` is used or not
-   Use it in `LlmChat` component
-   Keep in mind DRY (Don't Repeat Yourself) principle

---

[-]

[âœ¨ğŸ¨] qux

---

[-]

[âœ¨ğŸ¨] qux

---

[-]

[âœ¨ğŸ¨] qux
