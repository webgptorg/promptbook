import type { Promisable } from "type-fest";
import type { AvailableModel } from "../../../../execution/AvailableModel";
import type { LlmExecutionTools } from "../../../../execution/LlmExecutionTools";
import type { ChatPromptResult } from "../../../../execution/PromptResult";
import type { CompletionPromptResult } from "../../../../execution/PromptResult";
import type { EmbeddingPromptResult } from "../../../../execution/PromptResult";
import type { PromptResultUsage } from "../../../../execution/PromptResultUsage";
import { addUsage } from "../../../../execution/utils/addUsage";
import { ZERO_USAGE } from "../../../../execution/utils/usage-constants";
import type { ChatPrompt } from "../../../../types/Prompt";
import type { CompletionPrompt } from "../../../../types/Prompt";
import type { EmbeddingPrompt } from "../../../../types/Prompt";
import type { LlmExecutionToolsWithTotalUsage } from "./LlmExecutionToolsWithTotalUsage";

/**
 * Intercepts LLM tools and counts total usage of the tools
 *
 * @param llmTools LLM tools to be intercepted with usage counting
 * @returns LLM tools with same functionality with added total cost counting
 * @public exported from `@promptbook/core`
 */
export function countTotalUsage(
	llmTools: LlmExecutionTools,
): LlmExecutionToolsWithTotalUsage {
	let totalUsage: PromptResultUsage = ZERO_USAGE;

	const proxyTools: LlmExecutionToolsWithTotalUsage = {
		get title() {
			// TODO: [üß†] Maybe put here some suffix
			return llmTools.title;
		},

		get description() {
			// TODO: [üß†] Maybe put here some suffix
			return llmTools.description;
		},

		async checkConfiguration(): Promise<void> {
			return /* not await */ llmTools.checkConfiguration();
		},

		listModels(): Promisable<ReadonlyArray<AvailableModel>> {
			return /* not await */ llmTools.listModels();
		},

		getTotalUsage() {
			// <- Note: [ü•´] Not using getter `get totalUsage` but `getTotalUsage` to allow this object to be proxied
			return totalUsage;
		},
	};

	if (llmTools.callChatModel !== undefined) {
		proxyTools.callChatModel = async (
			prompt: ChatPrompt,
		): Promise<ChatPromptResult> => {
			// console.info('[üöï] callChatModel through countTotalUsage');
			const promptResult = await llmTools.callChatModel!(prompt);
			totalUsage = addUsage(totalUsage, promptResult.usage);
			return promptResult;
		};
	}

	if (llmTools.callCompletionModel !== undefined) {
		proxyTools.callCompletionModel = async (
			prompt: CompletionPrompt,
		): Promise<CompletionPromptResult> => {
			// console.info('[üöï] callCompletionModel through countTotalUsage');
			const promptResult = await llmTools.callCompletionModel!(prompt);
			totalUsage = addUsage(totalUsage, promptResult.usage);
			return promptResult;
		};
	}

	if (llmTools.callEmbeddingModel !== undefined) {
		proxyTools.callEmbeddingModel = async (
			prompt: EmbeddingPrompt,
		): Promise<EmbeddingPromptResult> => {
			// console.info('[üöï] callEmbeddingModel through countTotalUsage');
			const promptResult = await llmTools.callEmbeddingModel!(prompt);
			totalUsage = addUsage(totalUsage, promptResult.usage);
			return promptResult;
		};
	}

	// <- Note: [ü§ñ]

	return proxyTools;
}

/**
 * TODO: [üß†][üí∏] Maybe make some common abstraction `interceptLlmTools` and use here (or use javascript Proxy?)
 * TODO: [üß†] Is there some meaningfull way how to test this util
 * TODO: [üß†][üåØ] Maybe a way how to hide ability to `get totalUsage`
 *     > const [llmToolsWithUsage,getUsage] = countTotalUsage(llmTools);
 * TODO: [üë∑‚Äç‚ôÇÔ∏è] @@@ Manual about construction of llmTools
 */
