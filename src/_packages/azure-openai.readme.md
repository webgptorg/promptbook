`@promptbook/azure-openai` integrates [Azure OpenAI API](https://oai.azure.com/portal/) with [Promptbook](https://github.com/webgptorg/promptbook). It allows to execute Promptbooks with Azure OpenAI GPT models.

> Note: This is similar to [@promptbook/openai](https://www.npmjs.com/package/@promptbook/openai) but more useful for Enterprise customers who use Azure OpenAI to ensure strict data privacy and compliance.

## üß° Usage

```typescript
import { createPipelineExecutor } from '@promptbook/core';
import {
    createPipelineCollectionFromDirectory,
    $provideExecutionToolsForNode,
    $provideFilesystemForNode,
    $provideScrapersForNode,
    $provideScriptingForNode,
} from '@promptbook/node';
import { JavascriptExecutionTools } from '@promptbook/javascript';
import { AzureOpenAiExecutionTools } from '@promptbook/azure-openai';

// üõ† Prepare the tools that will be used to compile and run your books
// Note: Here you can allow or deny some LLM providers, such as not providing DeepSeek for privacy reasons
const fs = $provideFilesystemForNode();
const llm = new AzureOpenAiExecutionTools(
    //            <- TODO: [üß±] Implement in a functional (not new Class) way
    {
        isVerbose: true,
        resourceName: process.env.AZUREOPENAI_RESOURCE_NAME,
        deploymentName: process.env.AZUREOPENAI_DEPLOYMENT_NAME,
        apiKey: process.env.AZUREOPENAI_API_KEY,
    },
);
const executables = await $provideExecutablesForNode();
const tools = {
    llm,
    fs,
    scrapers: await $provideScrapersForNode({ fs, llm, executables }),
    script: await $provideScriptingForNode({}),
};

// ‚ñ∂ Create whole pipeline collection
const collection = await createPipelineCollectionFromDirectory('./books', tools);

// ‚ñ∂ Get single Pipeline
const pipeline = await collection.getPipelineByUrl(`https://promptbook.studio/my-collection/write-article.book`);

// ‚ñ∂ Create executor - the function that will execute the Pipeline
const pipelineExecutor = createPipelineExecutor({ pipeline, tools });

// ‚ñ∂ Prepare input parameters
const inputParameters = { word: 'crocodile' };

// üöÄ‚ñ∂ Execute the Pipeline
const result = await pipelineExecutor(inputParameters).asPromise({ isCrashedOnError: true });

// ‚ñ∂ Handle the result
const { isSuccessful, errors, outputParameters, executionReport } = result;
console.info(outputParameters);
```

<!--Import ./wizard.readme.md-->
<!--‚ö†Ô∏è WARNING: This section was imported, make changes in source; any manual changes here will be overwritten-->

## üßô‚Äç‚ôÇÔ∏è Wizard

Run books without any settings, boilerplate or struggle in Node.js:

```typescript
import { wizard } from '@promptbook/wizard';

const {
    outputParameters: { joke },
} = await wizard.execute(`https://github.com/webgptorg/book/blob/main/books/templates/generic.book`, {
    topic: 'Prague',
});

console.info(joke);
```

<!--/Import ./wizard.readme.md-->

<!--Import ./content/$provideExecutionToolsForNode.md-->
<!--‚ö†Ô∏è WARNING: This section was imported, make changes in source; any manual changes here will be overwritten-->

## üßô‚Äç‚ôÇÔ∏è Connect to LLM providers automatically

You can just use `$provideExecutionToolsForNode` function to create all required tools from environment variables like `ANTHROPIC_CLAUDE_API_KEY` and `OPENAI_API_KEY` automatically.

```typescript
import { createPipelineExecutor, createPipelineCollectionFromDirectory } from '@promptbook/core';
import { JavascriptExecutionTools } from '@promptbook/javascript';
import { $provideExecutionToolsForNode } from '@promptbook/node';
import { $provideFilesystemForNode } from '@promptbook/node';

// üõ† Prepare the tools that will be used to compile and run your books
// Note: Here you can allow or deny some LLM providers, such as not providing DeepSeek for privacy reasons
const tools = await $provideExecutionToolsForNode();

// ‚ñ∂ Create whole pipeline collection
const collection = await createPipelineCollectionFromDirectory('./books', tools);

// ‚ñ∂ Get single Pipeline
const pipeline = await collection.getPipelineByUrl(`https://promptbook.studio/my-collection/write-article.book`);

// ‚ñ∂ Create executor - the function that will execute the Pipeline
const pipelineExecutor = createPipelineExecutor({ pipeline, tools });

// ‚ñ∂ Prepare input parameters
const inputParameters = { word: 'dog' };

// üöÄ‚ñ∂ Execute the Pipeline
const result = await pipelineExecutor(inputParameters).asPromise({ isCrashedOnError: true });

// ‚ñ∂ Handle the result
const { isSuccessful, errors, outputParameters, executionReport } = result;
console.info(outputParameters);
```

<!--/Import ./content/$provideExecutionToolsForNode.md-->

## üíï Usage of multiple LLM providers

You can use multiple LLM providers in one Promptbook execution. The best model will be chosen automatically according to the prompt and the model's capabilities.

```typescript
import { createPipelineExecutor } from '@promptbook/core';
import {
    createPipelineCollectionFromDirectory,
    $provideExecutionToolsForNode,
    $provideFilesystemForNode,
} from '@promptbook/node';
import { JavascriptExecutionTools } from '@promptbook/javascript';
import { AzureOpenAiExecutionTools } from '@promptbook/azure-openai';
import { OpenAiExecutionTools } from '@promptbook/openai';
import { AnthropicClaudeExecutionTools } from '@promptbook/anthropic-claude';

// ‚ñ∂ Prepare multiple tools
const fs = $provideFilesystemForNode();
const llm = [
    // Note: You can use multiple LLM providers in one Promptbook execution.
    //       The best model will be chosen automatically according to the prompt and the model's capabilities.
    new AzureOpenAiExecutionTools(
        //            <- TODO: [üß±] Implement in a functional (not new Class) way
        {
            resourceName: process.env.AZUREOPENAI_RESOURCE_NAME,
            deploymentName: process.env.AZUREOPENAI_DEPLOYMENT_NAME,
            apiKey: process.env.AZUREOPENAI_API_KEY,
        },
    ),
    new OpenAiExecutionTools(
        //            <- TODO: [üß±] Implement in a functional (not new Class) way
        {
            apiKey: process.env.OPENAI_API_KEY,
        },
    ),
    new AnthropicClaudeExecutionTools(
        //            <- TODO: [üß±] Implement in a functional (not new Class) way
        {
            apiKey: process.env.ANTHROPIC_CLAUDE_API_KEY,
        },
    ),
];
const executables = await $provideExecutablesForNode();
const tools = {
    llm,
    fs,
    scrapers: await $provideScrapersForNode({ fs, llm, executables }),
    script: await $provideScriptingForNode({}),
};

// ‚ñ∂ Create whole pipeline collection
const collection = await createPipelineCollectionFromDirectory('./books', tools);

// ‚ñ∂ Get single Pipeline
const pipeline = await collection.getPipelineByUrl(`https://promptbook.studio/my-collection/write-article.book`);

// ‚ñ∂ Create executor - the function that will execute the Pipeline
const pipelineExecutor = createPipelineExecutor({ pipeline, tools });

// ‚ñ∂ Prepare input parameters
const inputParameters = { word: 'snake' };

// üöÄ‚ñ∂ Execute the Pipeline
const result = await pipelineExecutor(inputParameters).asPromise({ isCrashedOnError: true });

// ‚ñ∂ Handle the result
const { isSuccessful, errors, outputParameters, executionReport } = result;
console.info(outputParameters);
```

<!--Import ./content/providers.md-->
<!--‚ö†Ô∏è WARNING: This section was imported, make changes in source; any manual changes here will be overwritten-->

### üíô Integration with other models

See the other model integrations:

-   [OpenAI](https://www.npmjs.com/package/@promptbook/openai)
-   [Anthropic Claude](https://www.npmjs.com/package/@promptbook/anthropic-claude)
-   [Google Gemini](https://www.npmjs.com/package/@promptbook/google)
-   [Vercel](https://www.npmjs.com/package/@promptbook/vercel)
-   [Azure OpenAI](https://www.npmjs.com/package/@promptbook/azure-openai)

<!--/Import ./content/providers.md-->
